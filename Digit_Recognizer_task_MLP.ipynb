{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "IMPORT DATASET & LIBRARIES, HYPERPARAMETERS "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "\n",
    "data = pd.read_csv(\"mnist.csv\")\n",
    "\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [256, 10, 10, 10, 10, 10]\n",
    "output_size = 10\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "lr = 0.1\n",
    "momentum = 0.9\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NORMALIZING AND SPLIT DATASET INTO 80% TRAIN AND 20% FOR VALIDATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X = data.loc[:,data.columns != \"label\"].values/255   #Normalizing the values\n",
    "Y = data.label.values\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(X,Y,test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LOAD DATASET AS TENSORS AND ALSO SPLIT INTO BATCHES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(features_train)\n",
    "X_test = torch.from_numpy(features_test)\n",
    "\n",
    "Y_train = torch.from_numpy(targets_train).type(torch.LongTensor)\n",
    "Y_test = torch.from_numpy(targets_test).type(torch.LongTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(X_train,Y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test,Y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NETWORK SETUP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        #nn.init.normal_(self.fc1.weight, mean=0, std=0.01)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], output_size)\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        self.fc5 = nn.Linear(hidden_sizes[3], output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = self.dropout(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.softmax(output)\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LAST PARAMETERS AND MODEL TRAINING\n",
    "\n",
    "In general we tried to adjust the learning rate with 0.001 in fist but we saw that in very low values was very slow\n",
    "and in high values such as 1 it never converged. \n",
    "\n",
    "After we adjust the batch number and saw that in 10000 the accuracy was around 90% but at 10 it was very bad. So the\n",
    "optimal value was 100 because 1000 was also not very good in accuracy\n",
    "\n",
    "According to the network we tried to add 2 more hidden layers, 3 in total excluding the input/output layers. \n",
    "\n",
    "We noticed that even we put 1024 unit on each one the accuracy was 97% when the same accuracy was noticed with only one\n",
    "hidden layer with 256 units as the optimal number. \n",
    "\n",
    "We changed the gaussian weight initialization with xavier one and did not notice any difference.\n",
    "\n",
    "Finally we applied dropout in two layers at first but with one hidden layer once was applied with p=0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 0 - Training loss: 0.3644384476045995\n",
      "Epoch 1 - Training loss: 0.1634927003568831\n",
      "Epoch 2 - Training loss: 0.12229644579367538\n",
      "Epoch 3 - Training loss: 0.09980145037674487\n",
      "Epoch 4 - Training loss: 0.08852390619689933\n",
      "Epoch 5 - Training loss: 0.07646291952088921\n",
      "Epoch 6 - Training loss: 0.06579911293779074\n",
      "Epoch 7 - Training loss: 0.06163150298869221\n",
      "Epoch 8 - Training loss: 0.052137173629731436\n",
      "Epoch 9 - Training loss: 0.04847681830572696\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = Feedforward(input_size, hidden_sizes, output_size)\n",
    "\n",
    "model.double()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "\n",
    "        # And optimizes its weights here\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss / len(train_loader)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MODEL VALIDATION "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Number Of Images Tested = 8400\n\nModel Accuracy = 0.9753571428571428\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "for images, labels in test_loader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if (true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count / all_count))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}